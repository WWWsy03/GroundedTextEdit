/root/miniconda3/envs/qwenimage/lib/python3.12/site-packages/accelerate/utils/launch.py:447: UserWarning: Port `29500` is already in use. Accelerate will attempt to launch in a standalone-like mode by finding an open port automatically for this session. If this current attempt fails, or for more control in future runs, please specify a different port (e.g., `--main_process_port <your_chosen_port>`) or use `--main_process_port 0` for automatic selection in your launch command or Accelerate config file.
  warnings.warn(
Using RTX 4000 series which doesn't support faster communication speedups. Ensuring P2P and IB communications are disabled.
W1121 09:07:36.039000 1837231 site-packages/torch/distributed/run.py:803] 
W1121 09:07:36.039000 1837231 site-packages/torch/distributed/run.py:803] *****************************************
W1121 09:07:36.039000 1837231 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1121 09:07:36.039000 1837231 site-packages/torch/distributed/run.py:803] *****************************************
Multiple distributions found for package optimum. Picked distribution: optimum-quanto
Multiple distributions found for package optimum. Picked distribution: optimum-quanto
Multiple distributions found for package optimum. Picked distribution: optimum-quanto
Multiple distributions found for package optimum. Picked distribution: optimum-quanto
11/21/2025 09:07:43 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 2, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

11/21/2025 09:07:43 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 2, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

11/21/2025 09:07:43 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 2, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
11/21/2025 09:07:43 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 2, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

All model checkpoint weights were used when initializing AutoencoderKLQwenImage.

All the weights of AutoencoderKLQwenImage were initialized from the model checkpoint at /app/cold1/Qwen-Image-Edit-2509.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKLQwenImage for predictions without further training.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of /app/cold1/Qwen-Image-Edit-2509.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded tokenizer as Qwen2Tokenizer from `tokenizer` subfolder of /app/cold1/Qwen-Image-Edit-2509.
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00,  4.03it/s]Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.18it/s]Loaded processor as Qwen2VLProcessor from `processor` subfolder of /app/cold1/Qwen-Image-Edit-2509.
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.59it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A

Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:36<01:49, 36.45s/it][ALoading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:36<01:49, 36.46s/it][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:36<01:49, 36.45s/it][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:36<01:49, 36.55s/it][A

Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:22<01:24, 42.19s/it][ALoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:22<01:24, 42.19s/it][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:22<01:24, 42.19s/it][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:22<01:24, 42.26s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:43<00:32, 32.32s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:43<00:32, 32.32s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:43<00:32, 32.32s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:43<00:32, 32.33s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 24.25s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 28.78s/it]


Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 24.23s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 24.26s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 28.78s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 28.78s/it]

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 24.26s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:55<00:00, 28.78s/it]
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.05s/it]Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [02:00<08:00, 120.20s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:00<00:00, 37.68s/it]Loaded text_encoder as Qwen2_5_VLForConditionalGeneration from `text_encoder` subfolder of /app/cold1/Qwen-Image-Edit-2509.
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:00<00:00, 24.04s/it]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:00<00:00, 37.74s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:00<00:00, 24.04s/it]
ðŸ”¥å¼€å§‹çƒ­æ’æ‹” Attention Processors for Style Control...
âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° transformer.transformer_blocksã€‚çƒ­æ’æ‹”å¤±è´¥ã€‚
âœ… çƒ­æ’æ‹”å®Œæˆï¼
ðŸ”¥å¼€å§‹çƒ­æ’æ‹” Attention Processors for Style Control...
âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° transformer.transformer_blocksã€‚çƒ­æ’æ‹”å¤±è´¥ã€‚
âœ… çƒ­æ’æ‹”å®Œæˆï¼
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:00<00:27, 27.37s/it]Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [02:00<02:29, 49.93s/it] Finished precomputing embeddings.
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:01<00:00, 18.79s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:01<00:00, 24.21s/it]
ðŸ”¥å¼€å§‹çƒ­æ’æ‹” Attention Processors for Style Control...
âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° transformer.transformer_blocksã€‚çƒ­æ’æ‹”å¤±è´¥ã€‚
âœ… çƒ­æ’æ‹”å®Œæˆï¼
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<00:54, 27.21s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:01<00:00, 24.22s/it]
ðŸ”¥å¼€å§‹çƒ­æ’æ‹” Attention Processors for Style Control...
âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° transformer.transformer_blocksã€‚çƒ­æ’æ‹”å¤±è´¥ã€‚
âœ… çƒ­æ’æ‹”å®Œæˆï¼
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Finished precomputing embeddings.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Finished precomputing embeddings.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Finished precomputing embeddings.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]