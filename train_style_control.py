import argparse
import copy
import logging
import os
import math
import shutil
from pathlib import Path

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm
from PIL import Image
import numpy as np

from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration
import datasets
import diffusers
from diffusers import FlowMatchEulerDiscreteScheduler
from diffusers.optimization import get_scheduler
from diffusers.training_utils import (
    compute_density_for_timestep_sampling,
    compute_loss_weighting_for_sd3,
)
import transformers
from omegaconf import OmegaConf

from style_transfer_pipeline import QwenImageEditPlusPipelineWithStyleControl
from style_transfer_processor import QwenDoubleStreamAttnProcessor2_0WithStyleControl

from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.loaders import QwenImageLoraLoaderMixin
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from diffusers.models import AutoencoderKLQwenImage, QwenImageTransformer2DModel
from typing import Union, List, Optional, Dict, Any, Callable
import torch.nn as nn
from diffusers.utils.torch_utils import randn_tensor
from style_transfer_pipeline import (
    retrieve_latents,
    calculate_dimensions,
    CONDITION_IMAGE_SIZE, # å‡è®¾è¿™äº›å¸¸é‡åœ¨ä½ å¯¼å…¥æ—¶å¯ç”¨
    VAE_IMAGE_SIZE,       # å‡è®¾è¿™äº›å¸¸é‡åœ¨ä½ å¯¼å…¥æ—¶å¯ç”¨
    calculate_shift,
    retrieve_timesteps,
)
from style_transfer_processor import Attention
from diffusers.utils import is_torch_xla_available, replace_example_docstring
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm

    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False

from diffusers.image_processor import PipelineImageInput, VaeImageProcessor

import logging
logger = get_logger(__name__, log_level="INFO")


# ----------------------------------------
# è¾…åŠ©å‡½æ•° (æ¥è‡ªå‚è€ƒè„šæœ¬)
# ----------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(description="Train Qwen-Image-Edit with Style Control.")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        required=True,
        help="Path to the training config file (OmegaConf YAML).",
    )
    args = parser.parse_args()
    return args.config

# ä½ çš„ pipeline å’Œå‚è€ƒè„šæœ¬éƒ½ä¾èµ–è¿™ä¸ª
def calculate_dimensions(target_area, ratio):
    width = math.sqrt(target_area * ratio)
    height = width / ratio
    
    # Qwen-Image VAE éœ€è¦ 16*2 = 32 çš„å€æ•°
    multiple_of = 32 
    width = round(width / multiple_of) * multiple_of
    height = round(height / multiple_of) * multiple_of

    return width, height

# ----------------------------------------
# æ•°æ®é›†å®šä¹‰
# ----------------------------------------

class StyleEditDataset(Dataset):
    def __init__(self, train_data_dir, content_folder="content_images", style_folder="style_images", gt_folder="ground_truth_images", prompt_file="prompts.txt"):
        self.data_dir = Path(train_data_dir)
        self.content_dir = self.data_dir / content_folder
        self.style_dir = self.data_dir / style_folder
        self.gt_dir = self.data_dir / gt_folder
        prompt_path = self.data_dir / prompt_file

        if not all([self.content_dir.exists(), self.style_dir.exists(), self.gt_dir.exists(), prompt_path.exists()]):
            raise FileNotFoundError(f"Dataset directories not found in {train_data_dir}")

        # 1. åŠ è½½ Prompts
        with open(prompt_path, 'r', encoding='utf-8') as f:
            self.prompts = [line.strip() for line in f.readlines() if line.strip()]

        # 2. åŒ¹é…å›¾åƒ
        # å‡è®¾å›¾åƒæ–‡ä»¶åä¸€ä¸€å¯¹åº” (e.g., 001.jpg, 002.jpg)
        self.image_files = []
        for i in range(len(self.prompts)):
            # å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼Œå‡è®¾åŸºäºç´¢å¼•æˆ–å…±åŒçš„æ–‡ä»¶å
            # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ç¬¬ i è¡Œ prompt å¯¹åº”ç¬¬ i ä¸ªæ–‡ä»¶
            # ä½ éœ€è¦è°ƒæ•´è¿™ä¸ªé€»è¾‘ä»¥åŒ¹é…ä½ çš„æ–‡ä»¶å (e.g., img1.jpg, style1.jpg, gt1.jpg)
            
            # ç¤ºä¾‹ï¼šå‡è®¾æ–‡ä»¶åæ˜¯ 1.jpg, 2.jpg...
            # filename = f"{i+1}.jpg" 
            
            # ç¤ºä¾‹ï¼šå‡è®¾æ–‡ä»¶åä¸ prompt åˆ—è¡¨é¡ºåºä¸€è‡´
            # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ’åºè¿‡çš„æ–‡ä»¶åˆ—è¡¨
            content_files = sorted([f for f in self.content_dir.glob('*.jpg')] + [f for f in self.content_dir.glob('*.png')])
            style_files = sorted([f for f in self.style_dir.glob('*.jpg')] + [f for f in self.style_dir.glob('*.png')])
            gt_files = sorted([f for f in self.gt_dir.glob('*.jpg')] + [f for f in self.gt_dir.glob('*.png')])

            if i < len(content_files) and i < len(style_files) and i < len(gt_files):
                self.image_files.append({
                    "content": content_files[i],
                    "style": style_files[i],
                    "gt": gt_files[i]
                })
            else:
                logger.warning(f"Skipping index {i} due to missing image files.")

        if len(self.prompts) != len(self.image_files):
             logger.warning(f"Mismatch: {len(self.prompts)} prompts vs {len(self.image_files)} image sets. Truncating.")
             min_len = min(len(self.prompts), len(self.image_files))
             self.prompts = self.prompts[:min_len]
             self.image_files = self.image_files[:min_len]

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        try:
            prompt = self.prompts[idx]
            files = self.image_files[idx]
            
            content_img = Image.open(files["content"]).convert("RGB")
            style_img = Image.open(files["style"]).convert("RGB")
            gt_img = Image.open(files["gt"]).convert("RGB")
            print(f"Loaded data index {idx}: content {files['content'].name}, style {files['style'].name}, gt {files['gt'].name}")
            
            return content_img, style_img, gt_img, prompt
        except Exception as e:
            logger.error(f"Error loading data at index {idx}: {e}")
            # å°è¯•åŠ è½½ä¸‹ä¸€ä¸ª
            return self.__getitem__((idx + 1) % len(self))

def collate_fn(examples):
    content_images = [e[0] for e in examples]
    style_images = [e[1] for e in examples]
    gt_images = [e[2] for e in examples]
    prompts = [e[3] for e in examples]
    print(f"Collate batch size: {len(prompts)}")
    
    return {
        "content_images_pil": content_images,
        "style_images_pil": style_images,
        "gt_images_pil": gt_images,
        "prompts": prompts
    }

# ----------------------------------------
# ä¸»è®­ç»ƒå‡½æ•°
# ----------------------------------------

def main():
    config_path = parse_args()
    args = OmegaConf.load(config_path)

    logging_dir = os.path.join(args.output_dir, args.logging_dir)

    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
    )

    # æ—¥å¿—è®°å½•
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

    # è®¾ç½® DType
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        print("ä½¿ç”¨ fp16 è¿›è¡Œè®­ç»ƒ")
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        print("ä½¿ç”¨ bf16 è¿›è¡Œè®­ç»ƒ")
        weight_dtype = torch.bfloat16

    # --- 1. åŠ è½½è‡ªå®šä¹‰ Pipeline ---
    # QwenImageEditPlusPipelineWithStyleControl åº”è¯¥åœ¨ __init__ ä¸­
    # è‡ªåŠ¨è®¾ç½®å¥½è‡ªå®šä¹‰çš„ Processor
    try:
        pipeline = QwenImageEditPlusPipelineWithStyleControl.from_pretrained(
            args.pretrained_model_name_or_path,
            torch_dtype=weight_dtype,
            # local_files_only=True # å¦‚æœæœ¬åœ°å·²ç»ä¸‹è½½å¥½
        )
    except Exception as e:
        logger.error(f"åŠ è½½ pipeline å¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿ä½ çš„ QwenImageEditPlusPipelineWithStyleControl å’Œ QwenDoubleStreamAttnProcessor2_0WithStyleControl ç±»å·²æ­£ç¡®å®šä¹‰å¹¶å¯¼å…¥ã€‚")
        raise

    # --- 2. å†»ç»“ä¸»å¹²ç½‘ç»œï¼Œåªè®­ç»ƒ Processor çš„æ–°å±‚ ---
    logger.info("ğŸ”’ æ­£åœ¨å†»ç»“ä¸»å¹²ç½‘ç»œå‚æ•°...")
    # ä½ å¿…é¡»åˆ†åˆ«å†»ç»“ pipeline å†…çš„ nn.Module ç»„ä»¶
    pipeline.vae.requires_grad_(False)
    pipeline.text_encoder.requires_grad_(False)
    pipeline.transformer.requires_grad_(False)

    trainable_params = []
    total_blocks = 0
    if hasattr(pipeline, "transformer") and hasattr(pipeline.transformer, "transformer_blocks"):
        total_blocks = len(pipeline.transformer.transformer_blocks)
        for i, block in enumerate(pipeline.transformer.transformer_blocks):
            processor = block.attn.processor
            if isinstance(processor, QwenDoubleStreamAttnProcessor2_0WithStyleControl):
                # æ£€æŸ¥ processor æ˜¯å¦ä¸º nn.Module
                if not isinstance(processor, nn.Module):
                    raise TypeError(f"Block {i} çš„ Processor ä¸æ˜¯ nn.Moduleï¼è¯·ä¿®æ”¹ QwenDoubleStreamAttnProcessor2_0WithStyleControl ç»§æ‰¿ nn.Moduleã€‚")
                
                # è§£å†»å¹¶æ”¶é›†å‚æ•°
                for param_name, param in processor.named_parameters():
                    if "style_k_proj" in param_name or "style_v_proj" in param_name:
                        param.requires_grad = True
                        trainable_params.append(param)
                        logger.info(f"âœ… è§£å†»å‚æ•°: block_{i}.{param_name}")
            else:
                logger.warning(f"Block {i} çš„ processor ç±»å‹ä¸åŒ¹é…: {type(processor)}")
    
    if not trainable_params:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•å¯è®­ç»ƒçš„ 'style_k_proj' æˆ– 'style_v_proj' å‚æ•°ã€‚è¯·æ£€æŸ¥ä½ çš„ Processor å®ç°ã€‚")
    
    logger.info(f"âœ¨ æˆåŠŸè§£å†» {len(trainable_params)} ä¸ªå‚æ•°å¼ é‡ (æ¥è‡ª {total_blocks} ä¸ª blocks)ã€‚")
    logger.info(f"ğŸ’° å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in trainable_params) / 1_000_000:.2f} M")

    # ç§»åŠ¨åˆ°è®¾å¤‡ (åœ¨ prepare ä¹‹å‰)
    pipeline.to(accelerator.device)

    # --- 3. å‡†å¤‡ VAE å’Œ Scheduler (ç”¨äºè®­ç»ƒå¾ªç¯) ---
    # (vae å’Œ scheduler å·²ç»åŒ…å«åœ¨ pipeline ä¸­, å¹¶ä¸”è¢«å†»ç»“)
    vae = pipeline.vae
    noise_scheduler = pipeline.scheduler
    noise_scheduler_copy = copy.deepcopy(noise_scheduler)

    # VAE scale factor
    vae_scale_factor = 2 ** len(vae.temperal_downsample)
    
    # ä» VAE é…ç½®ä¸­è·å–
    latents_mean = (
        torch.tensor(vae.config.latents_mean)
        .view(1, 1, vae.config.z_dim, 1, 1)
        .to(accelerator.device, dtype=weight_dtype)
    )
    latents_std_inv = 1.0 / torch.tensor(vae.config.latents_std).view(1, 1, vae.config.z_dim, 1, 1).to(
        accelerator.device, dtype=weight_dtype
    )

    # (æ¥è‡ªå‚è€ƒè„šæœ¬)
    def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):
        sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)
        schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)
        timesteps = timesteps.to(accelerator.device)
        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
        
        sigma = sigmas[step_indices].flatten()
        while len(sigma.shape) < n_dim:
            sigma = sigma.unsqueeze(-1)
        return sigma

    # --- 4. ä¼˜åŒ–å™¨ ---
    optimizer_cls = torch.optim.AdamW
    optimizer = optimizer_cls(
        trainable_params, # åªä¼˜åŒ–æˆ‘ä»¬è§£å†»çš„å‚æ•°
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    # --- 5. æ•°æ®é›†å’Œ Dataloader ---
    train_dataset = StyleEditDataset(train_data_dir=args.data_config.train_data_dir)
    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        collate_fn=collate_fn,
        batch_size=args.train_batch_size,
        num_workers=args.dataloader_num_workers,
    )

    # --- 6. å­¦ä¹ ç‡è°ƒåº¦å™¨ ---
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        num_training_steps=args.max_train_steps * accelerator.num_processes,
    )

    # --- 7. Accelerator Prepare ---
    # æˆ‘ä»¬å‡†å¤‡æ•´ä¸ª pipelineï¼Œå› ä¸ºå®ƒåŒ…å«äº†å¯è®­ç»ƒçš„å­æ¨¡å—
    pipeline, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        pipeline, optimizer, train_dataloader, lr_scheduler
    )

    # --- 8. è®­ç»ƒå¾ªç¯ ---
    global_step = 0
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = (steps: {args.max_train_steps})")
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")

    progress_bar = tqdm(
        range(0, args.max_train_steps),
        initial=0,
        desc="Steps",
        disable=not accelerator.is_local_main_process,
    )

    # éœ€è¦ pipeline çš„ unwrap ç‰ˆæœ¬æ¥è°ƒç”¨é forward æ–¹æ³•
    unwrapped_pipeline = accelerator.unwrap_model(pipeline)
    
    # å›ºå®š VAE å’Œ Text Encoder ä¸ºè¯„ä¼°æ¨¡å¼
    unwrapped_pipeline.vae.eval()
    unwrapped_pipeline.text_encoder.eval()
    # Transformer ä¸»å¹²è¢«å†»ç»“ï¼Œä½†åŒ…å«å¯è®­ç»ƒå­æ¨¡å—ï¼Œåº”å¤„äº train æ¨¡å¼
    unwrapped_pipeline.transformer.train()

    # (æ¥è‡ªä½ çš„ pipeline __call__)
    # å‡è®¾è¿™äº›å¸¸é‡åœ¨å¯¼å…¥æ—¶å·²å®šä¹‰
    # å¦‚æœæ²¡æœ‰ï¼Œä½ éœ€è¦åœ¨è¿™é‡Œæˆ– config ä¸­å®šä¹‰å®ƒä»¬
    _CONDITION_IMAGE_SIZE = CONDITION_IMAGE_SIZE if "CONDITION_IMAGE_SIZE" in globals() else 1024*1024
    _VAE_IMAGE_SIZE = VAE_IMAGE_SIZE if "VAE_IMAGE_SIZE" in globals() else 1024*1024

    for epoch in range(args.num_train_epochs): # å‡è®¾ config ä¸­æœ‰ num_train_epochs
        train_loss = 0.0
        for step, batch in enumerate(train_dataloader):
            
            # --- 8.1 å‡†å¤‡æ•°æ® (åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­) ---
            with torch.no_grad():
                bsz = len(batch["prompts"])
                device = accelerator.device
                
                # --- a) ç›®æ ‡ (Ground Truth) Latents ---
                gt_images_pil = batch["gt_images_pil"]
                gt_pixel_values_list = []
                for img in gt_images_pil:
                    w, h = calculate_dimensions(_VAE_IMAGE_SIZE, img.size[0] / img.size[1])
                    # preprocess è¿”å› (C, H, W)ï¼Œéœ€è¦ (C, 1, H, W)
                    gt_pixel_values_list.append(
                        unwrapped_pipeline.image_processor.preprocess(img, h, w).unsqueeze(2)
                    )
                gt_pixel_values = torch.cat(gt_pixel_values_list, dim=0).to(device, dtype=weight_dtype)
                
                # pixel_latents: (B, C, 1, H_lat, W_lat)
                pixel_latents = unwrapped_pipeline.vae.encode(gt_pixel_values).latent_dist.sample()
                pixel_latents = (pixel_latents - latents_mean) * latents_std_inv # å½’ä¸€åŒ–
                
                target_height, target_width = pixel_latents.shape[3:]

                # --- b) å™ªå£°å’Œ Timesteps ---
                noise = torch.randn_like(pixel_latents, device=device, dtype=weight_dtype)
                u = compute_density_for_timestep_sampling(
                    weighting_scheme="none", # (æ¥è‡ªå‚è€ƒè„šæœ¬)
                    batch_size=bsz,
                    logit_mean=0.0,
                    logit_std=1.0,
                    mode_scale=1.29,
                )
                indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()
                timesteps = noise_scheduler_copy.timesteps[indices].to(device=device)
                
                sigmas = get_sigmas(timesteps, n_dim=pixel_latents.ndim, dtype=pixel_latents.dtype)
                
                # (B, C, 1, H_lat, W_lat)
                noisy_model_input = (1.0 - sigmas) * pixel_latents + sigmas * noise
                
                # Pack: (B, L_noise, C_packed)
                packed_noisy_model_input = unwrapped_pipeline._pack_latents(
                    noisy_model_input,
                    bsz, 
                    noisy_model_input.shape[2],
                    target_height,
                    target_width,
                )
                L_noise = packed_noisy_model_input.shape[1]

                # --- c) æ–‡æœ¬æ¡ä»¶ ---
                content_images_pil = batch["content_images_pil"]
                style_images_pil = batch["style_images_pil"]
                
                condition_images = [] # ç”¨äº text encoder
                for img_list in zip(content_images_pil, style_images_pil):
                    img_pair = []
                    for img in img_list:
                        w, h = calculate_dimensions(_CONDITION_IMAGE_SIZE, img.size[0] / img.size[1])
                        img_pair.append(unwrapped_pipeline.image_processor.resize(img, h, w))
                    condition_images.append(img_pair[0]) # å‡è®¾ encode_prompt åªéœ€ content
                    condition_images.append(img_pair[1]) # å‡è®¾ encode_prompt éœ€è¦ [content, style]
                
                # ä½ çš„ encode_prompt æ¥å— image åˆ—è¡¨
                # (B, L_txt, D_txt)
                prompt_embeds, prompt_embeds_mask = unwrapped_pipeline.encode_prompt(
                    image=condition_images, # [content1, style1, content2, style2, ...] ?
                    prompt=batch["prompts"],
                    device=device,
                    num_images_per_prompt=1,
                    max_sequence_length=unwrapped_pipeline.tokenizer_max_length,
                )
                txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()

                # --- d) å›¾åƒ/é£æ ¼ æ¡ä»¶ ---
                vae_images_list_content = []
                vae_images_list_style = []
                
                img_shapes_list = [] # ç”¨äº transformer

                for content_img, style_img in zip(content_images_pil, style_images_pil):
                    # Content
                    w_c, h_c = calculate_dimensions(_VAE_IMAGE_SIZE, content_img.size[0] / content_img.size[1])
                    vae_images_list_content.append(
                        unwrapped_pipeline.image_processor.preprocess(content_img, h_c, w_c).unsqueeze(2)
                    )
                    # Style
                    w_s, h_s = calculate_dimensions(_VAE_IMAGE_SIZE, style_img.size[0] / style_img.size[1])
                    vae_images_list_style.append(
                        unwrapped_pipeline.image_processor.preprocess(style_img, h_s, w_s).unsqueeze(2)
                    )
                    
                    # (H_lat, W_lat)
                    noise_shape = (target_height // 2, target_width // 2)
                    content_shape = (h_c // (vae_scale_factor * 2) // 2, w_c // (vae_scale_factor * 2) // 2)
                    style_shape = (h_s // (vae_scale_factor * 2) // 2, w_s // (vae_scale_factor * 2) // 2)
                    
                    # (æ¥è‡ªå‚è€ƒè„šæœ¬ï¼Œä½†ä¸º3éƒ¨åˆ†è°ƒæ•´)
                    img_shapes_list.append([(1, *noise_shape), (1, *content_shape), (1, *style_shape)])


                vae_images_content = torch.cat(vae_images_list_content, dim=0).to(device, dtype=weight_dtype)
                vae_images_style = torch.cat(vae_images_list_style, dim=0).to(device, dtype=weight_dtype)

                # ç¼–ç å¹¶ Pack
                content_latents = unwrapped_pipeline._encode_vae_image(vae_images_content, generator=None)
                style_latents_unpacked = unwrapped_pipeline._encode_vae_image(vae_images_style, generator=None)
                
                packed_content_latents = unwrapped_pipeline._pack_latents(
                    content_latents, bsz, content_latents.shape[1], content_latents.shape[3], content_latents.shape[4]
                )
                packed_style_latents = unwrapped_pipeline._pack_latents(
                    style_latents_unpacked, bsz, style_latents_unpacked.shape[1], style_latents_unpacked.shape[3], style_latents_unpacked.shape[4]
                )
                
                # (B, L_content + L_style, C_packed)
                image_latents = torch.cat([packed_content_latents, packed_style_latents], dim=1)
                
                L_content_patches = packed_content_latents.shape[1]
                L_style_patches = packed_style_latents.shape[1]
                
                # --- e) å‡†å¤‡ Kwargs ---
                attention_kwargs = {
                    "style_image_latents": packed_style_latents,
                    "style_start_idx": L_noise + L_content_patches,
                    "style_end_idx": L_noise + L_content_patches + L_style_patches,
                    "noise_patches_length": L_noise,
                    "content_patches_length": L_content_patches,
                    "style_scale": args.style_scale # ä» config è·å–
                }
                
                # --- f) æœ€ç»ˆè¾“å…¥ ---
                # (B, L_noise + L_content + L_style, C_packed)
                latent_model_input = torch.cat([packed_noisy_model_input, image_latents], dim=1)

            # --- 8.2 è®­ç»ƒæ­¥éª¤ (å¼€å¯æ¢¯åº¦) ---
            with accelerator.accumulate(pipeline):
                # (B, L_total, C_packed)
                model_pred = unwrapped_pipeline.transformer(
                    hidden_states=latent_model_input,
                    timestep=timesteps / 1000,
                    guidance=None,
                    encoder_hidden_states_mask=prompt_embeds_mask,
                    encoder_hidden_states=prompt_embeds,
                    img_shapes=img_shapes_list,
                    txt_seq_lens=txt_seq_lens,
                    attention_kwargs=attention_kwargs,
                    return_dict=False,
                )[0]
                
                # --- 8.3 Loss è®¡ç®— ---
                # åªå–å™ªå£°éƒ¨åˆ†çš„é¢„æµ‹
                model_pred = model_pred[:, :L_noise]
                
                # Unpack
                model_pred = unwrapped_pipeline._unpack_latents(
                    model_pred,
                    height=target_height * vae_scale_factor,
                    width=target_width * vae_scale_factor,
                    vae_scale_factor=vae_scale_factor,
                )
                
                # (æ¥è‡ªå‚è€ƒè„šæœ¬)
                weighting = compute_loss_weighting_for_sd3(weighting_scheme="none", sigmas=sigmas)
                
                # Flow-matching loss: ç›®æ ‡æ˜¯ (noise - pixel_latents)
                target = noise - pixel_latents
                
                # (B, C, 1, H, W) -> (B, 1, C, H, W)
                target = target.permute(0, 2, 1, 3, 4) 
                
                loss = torch.mean(
                    (weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1),
                    1,
                )
                loss = loss.mean()
                
                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
                train_loss += avg_loss.item() / args.gradient_accumulation_steps

                # Backpropagate
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    # è£å‰ªå¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦
                    accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # --- 8.4 æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ ---
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                accelerator.log({"train_loss": train_loss, "lr": lr_scheduler.get_last_lr()[0]}, step=global_step)
                train_loss = 0.0

                if global_step % args.checkpointing_steps == 0:
                    if accelerator.is_main_process:
                        save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                        os.makedirs(save_path, exist_ok=True)
                        
                        # åªä¿å­˜æˆ‘ä»¬è®­ç»ƒçš„å‚æ•°
                        current_unwrapped_pipeline = accelerator.unwrap_model(pipeline)
                        style_control_state_dict = {}
                        
                        # ä» unwrappedæ¨¡å‹ ä¸­æå–æƒé‡
                        for name, param in current_unwrapped_pipeline.named_parameters():
                            if param.requires_grad:
                                # key åº”è¯¥åŒ¹é…åŠ è½½æ—¶çš„ key
                                # e.g., 'transformer.transformer_blocks.0.attn.processor.style_k_proj.weight'
                                style_control_state_dict[name] = param.cpu().to(torch.float32).detach()
                        
                        if style_control_state_dict:
                            torch.save(style_control_state_dict, os.path.join(save_path, "style_control_weights.pth"))
                            logger.info(f"âœ… Saved style control weights to {save_path}/style_control_weights.pth")
                        else:
                            logger.warning("æœªæ‰¾åˆ°å¯ä¿å­˜çš„å·²è®­ç»ƒæƒé‡ã€‚")

            logs = {"step_loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            progress_bar.set_postfix(**logs)

            if global_step >= args.max_train_steps:
                break
        
        if global_step >= args.max_train_steps:
            break

    accelerator.wait_for_everyone()
    
    # --- 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    if accelerator.is_main_process:
        save_path = os.path.join(args.output_dir, "final_model")
        os.makedirs(save_path, exist_ok=True)
        
        current_unwrapped_pipeline = accelerator.unwrap_model(pipeline)
        style_control_state_dict = {}
        for name, param in current_unwrapped_pipeline.named_parameters():
            if param.requires_grad:
                style_control_state_dict[name] = param.cpu().to(torch.float32).detach()
        
        if style_control_state_dict:
            torch.save(style_control_state_dict, os.path.join(save_path, "style_control_weights.pth"))
            logger.info(f"âœ… Saved final style control weights to {save_path}/style_control_weights.pth")

    accelerator.end_training()


if __name__ == "__main__":
    main()