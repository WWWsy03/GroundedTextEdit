import argparse
import copy
import gc
import logging
import os
import math
import shutil
from pathlib import Path

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm
from PIL import Image
import numpy as np

from accelerate import Accelerator, init_empty_weights
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration
import datasets
import diffusers
from diffusers import FlowMatchEulerDiscreteScheduler
from diffusers.optimization import get_scheduler
from diffusers.training_utils import (
    compute_density_for_timestep_sampling,
    compute_loss_weighting_for_sd3,
)
import transformers
from omegaconf import OmegaConf

from style_transfer_pipeline import QwenImageEditPlusPipelineWithStyleControl
from style_transfer_processor import QwenDoubleStreamAttnProcessor2_0WithStyleControl

from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.loaders import QwenImageLoraLoaderMixin
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from diffusers.models import AutoencoderKLQwenImage, QwenImageTransformer2DModel
from typing import Union, List, Optional, Dict, Any, Callable
import torch.nn as nn
from diffusers.utils.torch_utils import randn_tensor
from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor
from style_transfer_pipeline import (
    retrieve_latents,
    calculate_dimensions,
    CONDITION_IMAGE_SIZE, # å‡è®¾è¿™äº›å¸¸é‡åœ¨ä½ å¯¼å…¥æ—¶å¯ç”¨
    VAE_IMAGE_SIZE,       # å‡è®¾è¿™äº›å¸¸é‡åœ¨ä½ å¯¼å…¥æ—¶å¯ç”¨
    calculate_shift,
    retrieve_timesteps,
)
from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor
from style_transfer_processor import Attention
from diffusers.utils import is_torch_xla_available, replace_example_docstring
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm

    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False

from diffusers.image_processor import PipelineImageInput, VaeImageProcessor

import logging
logger = get_logger(__name__, log_level="INFO")


# ----------------------------------------
# è¾…åŠ©å‡½æ•° (æ¥è‡ªå‚è€ƒè„šæœ¬)
# ----------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(description="Train Qwen-Image-Edit with Style Control.")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        required=True,
        help="Path to the training config file (OmegaConf YAML).",
    )
    args = parser.parse_args()
    return args.config

# ä½ çš„ pipeline å’Œå‚è€ƒè„šæœ¬éƒ½ä¾èµ–è¿™ä¸ª
def calculate_dimensions(target_area, ratio):
    width = math.sqrt(target_area * ratio)
    height = width / ratio
    
    # Qwen-Image VAE éœ€è¦ 16*2 = 32 çš„å€æ•°
    multiple_of = 32 
    width = round(width / multiple_of) * multiple_of
    height = round(height / multiple_of) * multiple_of

    return width, height

# ----------------------------------------
# æ•°æ®é›†å®šä¹‰
# ----------------------------------------

class StyleEditDataset(Dataset):
    def __init__(self, train_data_dir, content_folder="content_images", style_folder="style_images", gt_folder="ground_truth_images", prompt_file="prompts.txt"):
        self.data_dir = Path(train_data_dir)
        self.content_dir = self.data_dir / content_folder
        self.style_dir = self.data_dir / style_folder
        self.gt_dir = self.data_dir / gt_folder
        prompt_path = self.data_dir / prompt_file

        if not all([self.content_dir.exists(), self.style_dir.exists(), self.gt_dir.exists(), prompt_path.exists()]):
            raise FileNotFoundError(f"Dataset directories not found in {train_data_dir}")

        # 1. åŠ è½½ Prompts
        with open(prompt_path, 'r', encoding='utf-8') as f:
            self.prompts = [line.strip() for line in f.readlines() if line.strip()]

        # 2. åŒ¹é…å›¾åƒ
        # å‡è®¾å›¾åƒæ–‡ä»¶åä¸€ä¸€å¯¹åº” (e.g., 001.jpg, 002.jpg)
        self.image_files = []
        for i in range(len(self.prompts)):
            # å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼Œå‡è®¾åŸºäºç´¢å¼•æˆ–å…±åŒçš„æ–‡ä»¶å
            # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ç¬¬ i è¡Œ prompt å¯¹åº”ç¬¬ i ä¸ªæ–‡ä»¶
            # ä½ éœ€è¦è°ƒæ•´è¿™ä¸ªé€»è¾‘ä»¥åŒ¹é…ä½ çš„æ–‡ä»¶å (e.g., img1.jpg, style1.jpg, gt1.jpg)
            
            # ç¤ºä¾‹ï¼šå‡è®¾æ–‡ä»¶åæ˜¯ 1.jpg, 2.jpg...
            # filename = f"{i+1}.jpg" 
            
            # ç¤ºä¾‹ï¼šå‡è®¾æ–‡ä»¶åä¸ prompt åˆ—è¡¨é¡ºåºä¸€è‡´
            # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ’åºè¿‡çš„æ–‡ä»¶åˆ—è¡¨
            content_files = sorted([f for f in self.content_dir.glob('*.jpg')] + [f for f in self.content_dir.glob('*.png')])
            style_files = sorted([f for f in self.style_dir.glob('*.jpg')] + [f for f in self.style_dir.glob('*.png')])
            gt_files = sorted([f for f in self.gt_dir.glob('*.jpg')] + [f for f in self.gt_dir.glob('*.png')])

            if i < len(content_files) and i < len(style_files) and i < len(gt_files):
                self.image_files.append({
                    "content": content_files[i],
                    "style": style_files[i],
                    "gt": gt_files[i]
                })
            else:
                logger.warning(f"Skipping index {i} due to missing image files.")

        if len(self.prompts) != len(self.image_files):
             logger.warning(f"Mismatch: {len(self.prompts)} prompts vs {len(self.image_files)} image sets. Truncating.")
             min_len = min(len(self.prompts), len(self.image_files))
             self.prompts = self.prompts[:min_len]
             self.image_files = self.image_files[:min_len]

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        try:
            prompt = self.prompts[idx]
            files = self.image_files[idx]
            
            content_img = Image.open(files["content"]).convert("RGB")
            style_img = Image.open(files["style"]).convert("RGB")
            gt_img = Image.open(files["gt"]).convert("RGB")
            print(f"Loaded data index {idx}: content {files['content'].name}, style {files['style'].name}, gt {files['gt'].name}")
            
            return content_img, style_img, gt_img, prompt
        except Exception as e:
            logger.error(f"Error loading data at index {idx}: {e}")
            # å°è¯•åŠ è½½ä¸‹ä¸€ä¸ª
            return self.__getitem__((idx + 1) % len(self))

def collate_fn(examples):
    content_images = [e[0] for e in examples]
    style_images = [e[1] for e in examples]
    gt_images = [e[2] for e in examples]
    prompts = [e[3] for e in examples]
    print(f"Collate batch size: {len(prompts)}")
    
    return {
        "content_images_pil": content_images,
        "style_images_pil": style_images,
        "gt_images_pil": gt_images,
        "prompts": prompts
    }

# ----------------------------------------
# ä¸»è®­ç»ƒå‡½æ•°
# ----------------------------------------

def main():
    config_path = parse_args()
    args = OmegaConf.load(config_path)

    logging_dir = os.path.join(args.output_dir, args.logging_dir)

    # 1. åˆå§‹åŒ– Accelerator (DeepSpeed é…ç½®ä» `accelerate config` è‡ªåŠ¨è¯»å–)
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)

    # 1. åˆå§‹åŒ– Accelerator (æ ‡å‡† DDPï¼Œä¸éœ€è¦ DeepSpeed/FSDP)
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir),
    )

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%MS",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)

    if accelerator.is_main_process:
        os.makedirs(args.output_dir, exist_ok=True)

    # è®¾ç½® DType
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    else:
        weight_dtype = torch.float32

    # --- 2. åŠ è½½æ¨¡å‹ ---
    logger.info("Loading models...")
    
    tokenizer = Qwen2Tokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")
    processor = Qwen2VLProcessor.from_pretrained(args.pretrained_model_name_or_path, subfolder="processor")
    noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="scheduler"
    )
    
    # [å…³é”® 1] VAE åŠ è½½åˆ° GPU (å®ƒæ¯”è¾ƒå°ï¼Œä¸”é¢‘ç¹ä½¿ç”¨)
    vae = AutoencoderKLQwenImage.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", torch_dtype=weight_dtype
    ).to(accelerator.device)
    
    # [å…³é”® 2] Text Encoder å¼ºåˆ¶ä¿ç•™åœ¨ CPU
    # Qwen2.5-VL å¾ˆå¤§ï¼Œä¸ºäº†çœæ˜¾å­˜ï¼Œæˆ‘ä»¬è®©å®ƒç•™åœ¨ CPU ä¸Š
    text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", torch_dtype=weight_dtype
    ).to("cpu") 
    
    # Transformer åŠ è½½åˆ° GPU (æˆ‘ä»¬éœ€è¦è®­ç»ƒå®ƒ)
    transformer = QwenImageTransformer2DModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="transformer", torch_dtype=weight_dtype
    ).to(accelerator.device)

    # --- 3. ç»„è£… Pipeline ---
    # æ³¨æ„ï¼špipeline å†…éƒ¨ä¼šæŒæœ‰è¿™äº›æ¨¡å‹ã€‚
    # æ­¤æ—¶ pipeline.text_encoder åœ¨ CPUï¼Œå…¶ä»–åœ¨ GPUã€‚
    pipeline = QwenImageEditPlusPipelineWithStyleControl(
        scheduler=noise_scheduler,
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        processor=processor,
        transformer=transformer,
    )
    # åªæœ‰ pipeline è‡ªèº«ä¸éœ€è¦ .to(device)ï¼Œå› ä¸ºå®ƒæ··åˆäº†è®¾å¤‡

    # --- 4. å†»ç»“ & è§£å†» ---
    logger.info("ğŸ”’ æ­£åœ¨å†»ç»“ä¸»å¹²ç½‘ç»œå‚æ•°...")
    pipeline.vae.requires_grad_(False)
    pipeline.text_encoder.requires_grad_(False)
    pipeline.transformer.requires_grad_(False)
    
    trainable_params = []
    if hasattr(pipeline.transformer, "transformer_blocks"):
        for i, block in enumerate(pipeline.transformer.transformer_blocks):
            processor = block.attn.processor
            if isinstance(processor, QwenDoubleStreamAttnProcessor2_0WithStyleControl):
                if not isinstance(processor, nn.Module):
                     raise TypeError(f"Processor must be nn.Module")
                for param_name, param in processor.named_parameters():
                    if "style_k_proj" in param_name or "style_v_proj" in param_name:
                        param.requires_grad = True
                        trainable_params.append(param)
    
    if not trainable_params:
        raise ValueError("No trainable parameters found!")
    logger.info(f"ğŸ’° å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in trainable_params) / 1_000_000:.2f} M")

    # --- 5. ä¼˜åŒ–å™¨ ---
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    # --- 6. æ•°æ®é›† ---
    train_dataset = StyleEditDataset(train_data_dir=args.data_config.train_data_dir)
    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        collate_fn=collate_fn,
        batch_size=args.train_batch_size,
        num_workers=args.dataloader_num_workers,
    )

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        num_training_steps=args.max_train_steps * accelerator.num_processes,
    )

    # --- 7. Accelerator Prepare (æ ‡å‡† DDP) ---
    # æˆ‘ä»¬åªå‡†å¤‡éœ€è¦è®­ç»ƒçš„éƒ¨åˆ†ã€‚
    # VAE (GPU, Frozen) å’Œ TextEncoder (CPU, Frozen) ä¸éœ€è¦ prepare
    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        pipeline.transformer, optimizer, train_dataloader, lr_scheduler
    )
    
    # å›å¡« pipeline.transformer (å®ƒè¢« DDP åŒ…è£…äº†)
    pipeline.transformer = transformer

    # --- 8. è®­ç»ƒå‡†å¤‡ ---
    noise_scheduler_copy = copy.deepcopy(noise_scheduler)
    
    # config è·å–
    vae_config = pipeline.vae.config
    vae_scale_factor = 2 ** len(vae_config.temperal_downsample)
    latents_mean = torch.tensor(vae_config.latents_mean).view(1, 1, vae_config.z_dim, 1, 1).to(accelerator.device, dtype=weight_dtype)
    latents_std_inv = 1.0 / torch.tensor(vae_config.latents_std).view(1, 1, vae_config.z_dim, 1, 1).to(accelerator.device, dtype=weight_dtype)

    def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):
        sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)
        schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)
        timesteps = timesteps.to(accelerator.device)
        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
        sigma = sigmas[step_indices].flatten()
        while len(sigma.shape) < n_dim:
            sigma = sigma.unsqueeze(-1)
        return sigma

    # è®­ç»ƒå¾ªç¯
    global_step = 0
    logger.info("***** Running training (CPU Offload Strategy) *****")
    progress_bar = tqdm(range(0, args.max_train_steps), initial=0, desc="Steps", disable=not accelerator.is_local_main_process)

    # ç¡®ä¿æ¨¡å¼æ­£ç¡®
    pipeline.vae.eval()
    pipeline.text_encoder.eval()

    for epoch in range(args.num_train_epochs):
        train_loss = 0.0
        
        for step, batch in enumerate(train_dataloader):
            
            with torch.no_grad():
                bsz = len(batch["prompts"])
                device = accelerator.device # GPU
                
                # [1] Text Encoding (åœ¨ CPU ä¸Šè¿›è¡Œ)
                # æˆ‘ä»¬éœ€è¦æ„å»ºè¾“å…¥ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨ CPU ä¸Š
                # ä½ çš„ encode_prompt å¯èƒ½ä¼šè‡ªåŠ¨æŠŠè¾“å…¥ç§»åˆ° text_encoder çš„è®¾å¤‡ (CPU)
                # æˆ–è€…æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†ã€‚ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å¤„ç† condition_images çš„è®¾å¤‡ã€‚
                
                content_images_pil = batch["content_images_pil"]
                style_images_pil = batch["style_images_pil"]
                condition_images = []
                for img_list in zip(content_images_pil, style_images_pil):
                    img_pair = []
                    for img in img_list:
                        w, h = calculate_dimensions(CONDITION_IMAGE_SIZE, img.size[0] / img.size[1])
                        # resize è¿”å› PIL image æˆ– tensor
                        processed_img = pipeline.image_processor.resize(img, h, w)
                        img_pair.append(processed_img)
                    condition_images.append(img_pair[0])
                    condition_images.append(img_pair[1])

                # è°ƒç”¨ encode_promptã€‚å…³é”®æ˜¯ä¼ å…¥ device="cpu" (å¦‚æœ text_encoder éœ€è¦)
                # æˆ–è€…è®© pipeline è‡ªåŠ¨æ£€æµ‹ text_encoder.device
                # ä½ çš„ _get_qwen_prompt_embeds é»˜è®¤ç”¨ self.text_encoder.device
                # ä½†ä¸ºäº†ç¡®ä¿ç»“æœèƒ½å›åˆ° GPUï¼Œæˆ‘ä»¬åœ¨ encode_prompt è¿”å›åæ‰‹åŠ¨ .to(device)
                
                # æ³¨æ„ï¼šencode_prompt å†…éƒ¨å¯èƒ½è°ƒç”¨äº† pipeline.processor (Qwen2VLProcessor)
                # å®ƒé€šå¸¸å¤„ç† PIL å›¾ç‰‡ã€‚å¦‚æœä¼ å…¥ Tensorï¼Œéœ€ç¡®ä¿åœ¨ CPUã€‚
                
                # æ‰§è¡Œ CPU è®¡ç®—
                prompt_embeds_cpu, prompt_embeds_mask_cpu = pipeline.encode_prompt(
                    image=condition_images, 
                    prompt=batch["prompts"], 
                    device="cpu", # å¼ºåˆ¶åœ¨ CPU è®¡ç®—
                    num_images_per_prompt=1,
                    max_sequence_length=pipeline.tokenizer_max_length,
                )
                
                # [å…³é”®] å°†è®¡ç®—ç»“æœç§»å› GPU
                prompt_embeds = prompt_embeds_cpu.to(device)
                prompt_embeds_mask = prompt_embeds_mask_cpu.to(device)
                txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()

                # [2] VAE Encoding (åœ¨ GPU ä¸Šè¿›è¡Œï¼Œå› ä¸º VAE è¾ƒå°ä¸”åœ¨ GPU ä¸Š)
                gt_images_pil = batch["gt_images_pil"]
                gt_pixel_values_list = []
                for img in gt_images_pil:
                    w, h = calculate_dimensions(VAE_IMAGE_SIZE, img.size[0] / img.size[1])
                    gt_pixel_values_list.append(
                        pipeline.image_processor.preprocess(img, h, w).unsqueeze(2)
                    )
                gt_pixel_values = torch.cat(gt_pixel_values_list, dim=0).to(device, dtype=weight_dtype)
                
                pixel_latents = pipeline.vae.encode(gt_pixel_values).latent_dist.sample()
                pixel_latents = (pixel_latents - latents_mean) * latents_std_inv

                # [ä¿®å¤ 1] å¤„ç† 5D Latents (B, C, T, H, W) -> (B, C, H, W)
                # Qwen VAE æ˜¯ 3D çš„ï¼Œå¦‚æœæ˜¯å•å¼ å›¾ç‰‡ï¼ŒT=1ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ T ç»´åº¦æŒ¤æ‰
                if pixel_latents.ndim == 5:
                    pixel_latents = pixel_latents.squeeze(2) 
                
                # ç°åœ¨å®ƒæ˜¯ 4Dï¼Œshape[2] æ˜¯ Hï¼Œshape[3] æ˜¯ W
                target_height = pixel_latents.shape[2]
                target_width = pixel_latents.shape[3]

                # --- B. Noise & Timesteps ---
                noise = torch.randn_like(pixel_latents, device=device, dtype=weight_dtype)
                u = compute_density_for_timestep_sampling(
                    weighting_scheme="none", batch_size=bsz, logit_mean=0.0, logit_std=1.0, mode_scale=1.29,
                )
                indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()
                timesteps = noise_scheduler_copy.timesteps[indices].to(device=device)
                sigmas = get_sigmas(timesteps, n_dim=pixel_latents.ndim, dtype=pixel_latents.dtype)
                noisy_model_input = (1.0 - sigmas) * pixel_latents + sigmas * noise
                
                # [å…³é”®ä¿®å¤ 2] ä¼ å…¥ shape[1] ä½œä¸ºé€šé“æ•°
                packed_noisy_model_input = pipeline._pack_latents(
                    noisy_model_input, 
                    bsz, 
                    noisy_model_input.shape[1], # Channel
                    target_height, 
                    target_width,
                )
                L_noise = packed_noisy_model_input.shape[1]

                # --- C. Text Condition ---
                content_images_pil = batch["content_images_pil"]
                style_images_pil = batch["style_images_pil"]
                condition_images = []
                for img_list in zip(content_images_pil, style_images_pil):
                    img_pair = []
                    for img in img_list:
                        w, h = calculate_dimensions(CONDITION_IMAGE_SIZE, img.size[0] / img.size[1])
                        img_pair.append(pipeline.image_processor.resize(img, h, w))
                    condition_images.append(img_pair[0])
                    condition_images.append(img_pair[1])
                
                prompt_embeds, prompt_embeds_mask = pipeline.encode_prompt(
                    image=condition_images, prompt=batch["prompts"], device=device, num_images_per_prompt=1,
                    max_sequence_length=pipeline.tokenizer_max_length,
                )
                txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()

                # --- D. Style Condition ---
                vae_images_list_content = []
                vae_images_list_style = []
                img_shapes_list = []

                for content_img, style_img in zip(content_images_pil, style_images_pil):
                    w_c, h_c = calculate_dimensions(VAE_IMAGE_SIZE, content_img.size[0] / content_img.size[1])
                    vae_images_list_content.append(pipeline.image_processor.preprocess(content_img, h_c, w_c).unsqueeze(2))
                    
                    w_s, h_s = calculate_dimensions(VAE_IMAGE_SIZE, style_img.size[0] / style_img.size[1])
                    vae_images_list_style.append(pipeline.image_processor.preprocess(style_img, h_s, w_s).unsqueeze(2))
                    
                    # RoPE éœ€è¦åŸå§‹åˆ†è¾¨ç‡çš„æ¯”ä¾‹ï¼Œè¿™é‡Œè®¡ç®—ä¸€ä¸‹
                    noise_shape = (target_height // 2, target_width // 2)
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ ¹æ® latent çš„ç¼©æ”¾å› å­è®¡ç®—
                    content_shape = (h_c // (vae_scale_factor * 2) // 2, w_c // (vae_scale_factor * 2) // 2)
                    style_shape = (h_s // (vae_scale_factor * 2) // 2, w_s // (vae_scale_factor * 2) // 2)
                    img_shapes_list.append([(1, *noise_shape), (1, *content_shape), (1, *style_shape)])

                vae_images_content = torch.cat(vae_images_list_content, dim=0).to(device, dtype=weight_dtype)
                vae_images_style = torch.cat(vae_images_list_style, dim=0).to(device, dtype=weight_dtype)

                content_latents = pipeline._encode_vae_image(vae_images_content, generator=None)
                style_latents_unpacked = pipeline._encode_vae_image(vae_images_style, generator=None)
                
                # [å…³é”®ä¿®å¤ 3] åŒæ ·å¤„ç† Style Latents çš„ 5D -> 4D
                if content_latents.ndim == 5:
                    content_latents = content_latents.squeeze(2)
                if style_latents_unpacked.ndim == 5:
                    style_latents_unpacked = style_latents_unpacked.squeeze(2)

                packed_content_latents = pipeline._pack_latents(
                    content_latents, bsz, content_latents.shape[1], content_latents.shape[2], content_latents.shape[3]
                )
                packed_style_latents = pipeline._pack_latents(
                    style_latents_unpacked, bsz, style_latents_unpacked.shape[1], style_latents_unpacked.shape[2], style_latents_unpacked.shape[3]
                )
                
                image_latents = torch.cat([packed_content_latents, packed_style_latents], dim=1)
                L_content_patches = packed_content_latents.shape[1]
                L_style_patches = packed_style_latents.shape[1]
                
                attention_kwargs = {
                    "style_image_latents": packed_style_latents,
                    "style_start_idx": L_noise + L_content_patches,
                    "style_end_idx": L_noise + L_content_patches + L_style_patches,
                    "noise_patches_length": L_noise,
                    "content_patches_length": L_content_patches,
                    "style_scale": args.style_scale
                }
                
                latent_model_input = torch.cat([packed_noisy_model_input, image_latents], dim=1)
                # ç¡®ä¿éœ€è¦æ¢¯åº¦ (é’ˆå¯¹æŸäº›ç‰¹å®šçš„ DeepSpeed é…ç½®)
                latent_model_input.requires_grad_(True)

            # --- 9.2 è®­ç»ƒæ­¥éª¤ (Gradient) ---
            with accelerator.accumulate(pipeline.transformer):
                model_pred = pipeline.transformer(
                    hidden_states=latent_model_input,
                    timestep=timesteps / 1000,
                    guidance=None,
                    encoder_hidden_states_mask=prompt_embeds_mask,
                    encoder_hidden_states=prompt_embeds,
                    img_shapes=img_shapes_list,
                    txt_seq_lens=txt_seq_lens,
                    attention_kwargs=attention_kwargs,
                    return_dict=False,
                )[0]
                
                # Loss
                model_pred = model_pred[:, :L_noise]
                model_pred = pipeline._unpack_latents(
                    model_pred, 
                    height=target_height * vae_scale_factor, 
                    width=target_width * vae_scale_factor, 
                    vae_scale_factor=vae_scale_factor,
                )
                
                weighting = compute_loss_weighting_for_sd3(weighting_scheme="none", sigmas=sigmas)
                target = noise - pixel_latents
                
                # target ä¹Ÿæ˜¯ 4D (B, C, H, W)
                
                loss = torch.mean(
                    (weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1), 1,
                )
                loss = loss.mean()
                
                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
                train_loss += avg_loss.item() / args.gradient_accumulation_steps

                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # --- 9.3 æ—¥å¿—å’Œä¿å­˜ ---
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                accelerator.log({"train_loss": train_loss, "lr": lr_scheduler.get_last_lr()[0]}, step=global_step)
                train_loss = 0.0

                if global_step % args.checkpointing_steps == 0:
                    if accelerator.is_main_process:
                        save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                        os.makedirs(save_path, exist_ok=True)
                        
                        unwrapped_transformer = accelerator.unwrap_model(pipeline.transformer)
                        style_control_state_dict = {}
                        for name, param in unwrapped_transformer.named_parameters():
                            if param.requires_grad:
                                style_control_state_dict[name] = param.cpu().to(torch.float32).detach()
                        
                        if style_control_state_dict:
                            torch.save(style_control_state_dict, os.path.join(save_path, "style_control_weights.pth"))
                            logger.info(f"âœ… Saved weights to {save_path}")

            if global_step >= args.max_train_steps:
                break
        if global_step >= args.max_train_steps:
            break
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        save_path = os.path.join(args.output_dir, "final_model")
        os.makedirs(save_path, exist_ok=True)
        unwrapped_transformer = accelerator.unwrap_model(pipeline.transformer)
        style_control_state_dict = {}
        for name, param in unwrapped_transformer.named_parameters():
            if param.requires_grad:
                style_control_state_dict[name] = param.cpu().to(torch.float32).detach()
        if style_control_state_dict:
            torch.save(style_control_state_dict, os.path.join(save_path, "style_control_weights.pth"))
            logger.info("âœ… Saved final weights.")

    accelerator.end_training()

if __name__ == "__main__":
    main()