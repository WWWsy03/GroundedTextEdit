import argparse
import gc
import logging
import os
import math
import random
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.auto import tqdm
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration, set_seed
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_density_for_timestep_sampling, compute_loss_weighting_for_sd3
from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor
# å¼•å…¥ä½ çš„ Pipeline å’Œ Processor ç±» (å‡è®¾å®ƒä»¬å®šä¹‰åœ¨ model_utils.py æˆ–è€…ç›´æ¥ç²˜è´´åœ¨åŒä¸€ä¸ªæ–‡ä»¶)
# ä¸ºäº†æ¼”ç¤ºï¼Œè¿™é‡Œå‡è®¾ç”¨æˆ·å·²ç»å®šä¹‰äº†è¿™ä¸¤ä¸ªç±»ï¼Œæˆ–è€…ç›´æ¥ç²˜è´´åœ¨ä»£ç æœ€ä¸Šæ–¹
# from model_utils import QwenImageEditPlusPipelineWithStyleControl, QwenDoubleStreamAttnProcessor2_0WithStyleControl
# *** è¯·ç¡®ä¿å°†ä½ æä¾›çš„ Pipeline å’Œ Processor ç±»ä»£ç åŒ…å«åœ¨è¿è¡Œç¯å¢ƒä¸­ ***

from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from diffusers.models import AutoencoderKLQwenImage, QwenImageTransformer2DModel
from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor
from style_transfer_pipeline import QwenImageEditPlusPipelineWithStyleControl
from style_transfer_processor import QwenDoubleStreamAttnProcessor2_0WithStyleControl

logger = get_logger(__name__)

# ==============================================================================
# 2. è¾…åŠ©å‡½æ•°
# ==============================================================================

def calculate_dimensions(target_area, ratio):
    # å¤ç”¨ pipeline é€»è¾‘ä¸­ä¼šç”¨åˆ°çš„å°ºå¯¸è®¡ç®—
    width = math.sqrt(target_area * ratio)
    height = width / ratio
    width = round(width / 32) * 32
    height = round(height / 32) * 32
    return width, height

# ==============================================================================
# 3. æ•°æ®é›†å®šä¹‰
# ==============================================================================

class StyleControlDataset(Dataset):
    def __init__(self, data_root, embeds_dir):
        self.data_root = Path(data_root)
        self.prompts_file = self.data_root / "prompts.txt"
        self.embeds_dir = Path(embeds_dir)
        
        # ç®€å•çš„æ–‡ä»¶è¯»å–é€»è¾‘ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ’åºæˆ–åŒ¹é…é€»è¾‘
        with open(self.prompts_file, "r", encoding="utf-8") as f:
            self.prompts = [line.strip() for line in f.readlines()]

        self.content_files = sorted(list((self.data_root / "content_images").glob("*")))
        self.style_files = sorted(list((self.data_root / "style_images").glob("*")))
        self.gt_files = sorted(list((self.data_root / "ground_truth_images").glob("*")))

        assert len(self.content_files) == len(self.prompts), "Content images and prompts mismatch"

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        # ç›´æ¥è¿”å› PIL Imageï¼Œè®© Pipeline çš„ image_processor å»å¤„ç† Resize å’Œ Norm
        # è¿™æ ·æœ€å®‰å…¨ï¼Œå®Œå…¨éµå¾ª Pipeline çš„é¢„å¤„ç†é€»è¾‘
        content_img = Image.open(self.content_files[idx]).convert("RGB")
        style_img = Image.open(self.style_files[idx]).convert("RGB")
        gt_img = Image.open(self.gt_files[idx]).convert("RGB")

        # åŠ è½½é¢„è®¡ç®—çš„ Embeddings
        embed_path = self.embeds_dir / f"{idx}.pt"
        saved_embeds = torch.load(embed_path)

        return {
            "content_pil": content_img,
            "style_pil": style_img,
            "gt_pil": gt_img,
            "prompt_embeds": saved_embeds["prompt_embeds"],
            "prompt_embeds_mask": saved_embeds["prompt_embeds_mask"],
        }

    def collate_fn(self, examples):
        # ç®€å•çš„ collateï¼ŒæŠŠ PIL image ç»„æˆ listï¼ŒTensor stack èµ·æ¥
        batch = {
            "content_pil": [example["content_pil"] for example in examples],
            "style_pil": [example["style_pil"] for example in examples],
            "gt_pil": [example["gt_pil"] for example in examples],
            "prompt_embeds": torch.stack([example["prompt_embeds"] for example in examples]),
            "prompt_embeds_mask": torch.stack([example["prompt_embeds_mask"] for example in examples]),
        }
        return batch

# ==============================================================================
# 4. é¢„è®¡ç®—é€»è¾‘ (Pre-computation)
# ==============================================================================

def precompute_embeddings(args, accelerator):
    """
    ä½¿ç”¨ 'æ®‹è¡€ç‰ˆ' Pipeline (ä»…åŠ è½½ Text Encoder) è®¡ç®— Prompt Embeddings å¹¶ç¼“å­˜ã€‚
    å®Œå…¨å¤ç”¨ Pipeline çš„ encode_prompt é€»è¾‘ã€‚
    """
    if os.path.exists(args.precomputed_dir) and len(os.listdir(args.precomputed_dir)) > 0:
        logger.info("Found precomputed embeddings. Skipping...")
        return

    logger.info("Starting precomputation...")
    os.makedirs(args.precomputed_dir, exist_ok=True)

    weight_dtype = torch.float32
    if args.mixed_precision == "fp16": weight_dtype = torch.float16
    elif args.mixed_precision == "bf16": weight_dtype = torch.bfloat16

    # 1. åŠ è½½æ–‡æœ¬ç›¸å…³æ¨¡å‹
    text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", torch_dtype=weight_dtype
    )
    tokenizer = Qwen2Tokenizer.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="tokenizer"
    )
    processor = Qwen2VLProcessor.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="processor"
    )

    # 2. å®ä¾‹åŒ– Pipeline (Transformer å’Œ VAE ä¼  Noneï¼ŒèŠ‚çœæ˜¾å­˜)
    #    è¿™æ ·æˆ‘ä»¬å¯ä»¥è°ƒç”¨ encode_prompt, check_inputs, image_processor ç­‰é€»è¾‘
    pipeline = QwenImageEditPlusPipelineWithStyleControl(
        scheduler=None, # ä¸ä¸éœ€è¦
        vae=None,       # ä¸éœ€è¦
        transformer=None, # ä¸éœ€è¦
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        processor=processor
    )
    pipeline.to(accelerator.device)

    # 3. å‡†å¤‡æ•°æ®éå†
    prompts_file = os.path.join(args.train_data_dir, "prompts.txt")
    with open(prompts_file, "r", encoding="utf-8") as f:
        prompts = [line.strip() for line in f.readlines()]
    
    content_dir = os.path.join(args.train_data_dir, "content_images")
    content_files = sorted(list(Path(content_dir).glob("*")))

    # 4. å¾ªç¯è®¡ç®—
    for idx, (prompt, img_path) in enumerate(tqdm(zip(prompts, content_files), total=len(prompts))):
        pil_img = Image.open(img_path).convert("RGB")
        
        # é‡è¦ï¼šæ¨¡æ‹Ÿ __call__ ä¸­çš„é¢„å¤„ç†é€»è¾‘
        # Pipeline çš„ encode_prompt éœ€è¦æ¥æ”¶ç»è¿‡ resize çš„å›¾åƒï¼Œ
        # è™½ç„¶ encode_prompt å†…éƒ¨ä¹Ÿä¼šå¤„ç†ï¼Œä½†ä¸ºäº†å’Œæ¨ç†æ—¶ __call__ çš„è¡Œä¸ºä¸€è‡´ï¼š
        # __call__ ä¸­æ˜¯å…ˆç®— condition_width/height -> resize -> ä¼ ç»™ encode_prompt
        
        image_width, image_height = pil_img.size
        # è¿™é‡Œ 1024*1024 åº”è¯¥åšæˆå‚æ•°
        CONDITION_IMAGE_SIZE = args.resolution * args.resolution 
        condition_width, condition_height = calculate_dimensions(
            CONDITION_IMAGE_SIZE, image_width / image_height
        )
        
        # ä½¿ç”¨ pipeline è‡ªå¸¦çš„ image_processor è¿›è¡Œ resize
        # è¿™ç¡®ä¿äº†æ’å€¼æ–¹æ³•ç­‰ç»†èŠ‚ä¸€è‡´
        condition_image = pipeline.image_processor.resize(pil_img, condition_height, condition_width)
        
        with torch.no_grad():
            # è°ƒç”¨ Pipeline åŸç”Ÿæ–¹æ³•
            prompt_embeds, prompt_embeds_mask = pipeline.encode_prompt(
                prompt=prompt,
                image=[condition_image], # encode_prompt æœŸæœ› list æˆ– tensor
                device=accelerator.device,
                num_images_per_prompt=1,
                max_sequence_length=args.max_sequence_length
            )
        
        # ä¿å­˜
        torch.save({
            "prompt_embeds": prompt_embeds.cpu().squeeze(0), # remove batch dim [L, D]
            "prompt_embeds_mask": prompt_embeds_mask.cpu().squeeze(0)
        }, os.path.join(args.precomputed_dir, f"{idx}.pt"))

    # 5. é‡Šæ”¾æ˜¾å­˜
    del pipeline
    del text_encoder
    del processor
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    logger.info("Precomputation finished & Memory cleared.")

# ==============================================================================
# 5. ä¸»è®­ç»ƒé€»è¾‘
# ==============================================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pretrained_model_name_or_path", type=str, default="/app/cold1/Qwen-Image-Edit-2509")
    parser.add_argument("--train_data_dir", type=str, default="/app/code/texteditRoPE/train_data_dir")
    parser.add_argument("--output_dir", type=str, default="/app/code/texteditRoPE/qwenimage-style-control-output")
    parser.add_argument("--precomputed_dir", type=str, default="/app/code/texteditRoPE/train_data_dir/cached_embeddings")
    parser.add_argument("--resolution", type=int, default=1024)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--train_batch_size", type=int, default=1)
    parser.add_argument("--max_train_steps", type=int, default=1000)
    parser.add_argument("--checkpointing_steps", type=int, default=500)
    parser.add_argument("--mixed_precision", type=str, default="bf16")
    parser.add_argument("--max_sequence_length", type=int, default=1024)
    args = parser.parse_args()

    # åˆå§‹åŒ– Accelerator
    project_config = ProjectConfiguration(project_dir=args.output_dir)
    accelerator = Accelerator(
        mixed_precision=args.mixed_precision,
        gradient_accumulation_steps=1,
        project_config=project_config
    )
    set_seed(0)

    # -------------------------------------------------------
    # Phase 1: é¢„è®¡ç®— (Text Encoder)
    # -------------------------------------------------------
    if accelerator.is_main_process:
        precompute_embeddings(args, accelerator)
    accelerator.wait_for_everyone()
    print("âœ… Pre-computation phase completed.")

    # -------------------------------------------------------
    # Phase 2: å‡†å¤‡è®­ç»ƒæ¨¡å‹
    # -------------------------------------------------------
    weight_dtype = torch.float32
    if args.mixed_precision == "fp16": weight_dtype = torch.float16
    elif args.mixed_precision == "bf16": weight_dtype = torch.bfloat16

    # åŠ è½½ VAE, Scheduler, Transformer
    vae = AutoencoderKLQwenImage.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", torch_dtype=weight_dtype
    )
    noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="scheduler"
    )
    transformer = QwenImageTransformer2DModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="transformer", torch_dtype=weight_dtype
    )
    num_channels_latents = transformer.config['in_channels'] // 4

    # å†»ç»“ä¸»å¹²
    vae.requires_grad_(False)
    transformer.requires_grad_(False)

    # å®ä¾‹åŒ– "è®­ç»ƒç‰ˆ" Pipeline
    # ç›®çš„ï¼šä½¿ç”¨å…¶å†…éƒ¨ helper æ–¹æ³• (_encode_vae_image, _pack_latents, prepare_latents ç­‰)
    # Text Encoder ä¼  None ä»¥èŠ‚çœæ˜¾å­˜
    pipeline = QwenImageEditPlusPipelineWithStyleControl(
        scheduler=noise_scheduler,
        vae=vae,
        transformer=transformer,
        text_encoder=None, # å·²é¢„è®¡ç®—
        tokenizer=None,
        processor=None
    )
    # å°† pipeline ç§»åŠ¨åˆ° device (ä¸»è¦æ˜¯ä¸ºäº†å†…éƒ¨æ¨¡å—å¦‚ VAE èƒ½åœ¨æ­£ç¡®çš„ device ä¸Š)
    pipeline.to(accelerator.device)
    num_channels_latents = transformer.config['in_channels'] // 4 # **ä¿®æ­£: ä½¿ç”¨å­—å…¸è®¿é—®**
    # -------------------------------------------------------
    # Phase 3: çƒ­æ’æ‹” Processor
    # -------------------------------------------------------
    if hasattr(transformer.config, 'hidden_size'):
        style_hidden_dim = transformer.config.hidden_size
    else:
        style_hidden_dim = transformer.config.num_attention_heads * transformer.config.attention_head_dim
    
    style_context_dim = 16 * 4 # 64

    trainable_params = []
    print("ğŸ”¥ Injecting Trainable Processors...")
    
    for block in transformer.transformer_blocks:
        # å®ä¾‹åŒ–ä½ çš„ Processor
        processor = QwenDoubleStreamAttnProcessor2_0WithStyleControl(
            style_context_dim=style_context_dim,
            style_hidden_dim=style_hidden_dim
        )
        block.attn.processor = processor
        
        # å¼€å¯ KV è®­ç»ƒ
        processor.style_k_proj.requires_grad_(True)
        processor.style_v_proj.requires_grad_(True)
        
        trainable_params.extend(processor.style_k_proj.parameters())
        trainable_params.extend(processor.style_v_proj.parameters())

    # -------------------------------------------------------
    # Phase 4: ä¼˜åŒ–å™¨ä¸æ•°æ®
    # -------------------------------------------------------
    optimizer = torch.optim.AdamW(trainable_params, lr=args.learning_rate)
    
    dataset = StyleControlDataset(args.train_data_dir, embeds_dir=args.precomputed_dir)
    dataloader = DataLoader(
        dataset, 
        batch_size=args.train_batch_size, 
        shuffle=True, 
        collate_fn=dataset.collate_fn,
        num_workers=4
    )

    # Accelerator Prepare
    # æ³¨æ„ï¼šPipeline ä¸æ˜¯ torch.nn.Module çš„æ ‡å‡†å­ç±»ï¼Œä¸èƒ½è¢« prepareã€‚
    # æˆ‘ä»¬ prepare transformer å’Œ optimizerã€‚
    transformer, optimizer, dataloader = accelerator.prepare(
        transformer, optimizer, dataloader
    )

    # -------------------------------------------------------
    # Phase 5: è®­ç»ƒå¾ªç¯
    # -------------------------------------------------------
    global_step = 0
    transformer.train()
    
    # è·å– VAE scale factor (å¤ç”¨ pipeline é€»è¾‘)
    vae_scale_factor = pipeline.vae_scale_factor
    VAE_IMAGE_SIZE = args.resolution * args.resolution

    # è¾…åŠ©å‡½æ•°ï¼šè·å– Sigmas
    def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):
        sigmas = noise_scheduler.sigmas.to(device=accelerator.device, dtype=dtype)
        schedule_timesteps = noise_scheduler.timesteps.to(accelerator.device)
        timesteps = timesteps.to(accelerator.device)
        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
        sigma = sigmas[step_indices].flatten()
        while len(sigma.shape) < n_dim:
            sigma = sigma.unsqueeze(-1)
        return sigma

    print(f"Start Training. Params: {sum(p.numel() for p in trainable_params)/1e6:.2f}M")

    for epoch in range(10000):
        for batch in dataloader:
            with accelerator.accumulate(transformer):
                
                # 1. æ•°æ®å‡†å¤‡ (PIL -> Resize/Process -> Tensor)
                content_imgs = batch["content_pil"]
                style_imgs = batch["style_pil"]
                gt_imgs = batch["gt_pil"]
                
                processed_vae_images_content = []
                processed_vae_images_style = []
                processed_vae_images_gt = []
                
                bsz = len(content_imgs)
                
                VAE_IMAGE_SIZE = args.resolution * args.resolution 

                for i in range(bsz):
                    w, h = content_imgs[i].size
                    vae_w, vae_h = calculate_dimensions(VAE_IMAGE_SIZE, w/h)
                    
                    # VaeImageProcessor.preprocess è¾“å‡º [C, H, W]
                    # æ’å…¥ T=1 ç»´åº¦ï¼Œä½¿ç”¨ unsqueeze(1) å¾—åˆ° [C, T=1, H, W]
                    p_c = pipeline.image_processor.preprocess(content_imgs[i], vae_h, vae_w).unsqueeze(1)
                    p_s = pipeline.image_processor.preprocess(style_imgs[i], vae_h, vae_w).unsqueeze(1)
                    p_g = pipeline.image_processor.preprocess(gt_imgs[i], vae_h, vae_w).unsqueeze(1)
                    
                    processed_vae_images_content.append(p_c)
                    processed_vae_images_style.append(p_s)
                    processed_vae_images_gt.append(p_g)
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥å•å¼ å›¾ç‰‡å¼ é‡æ˜¯å¦æ˜¯ [C, T, H, W] (ä¾‹å¦‚ [3, 1, 1024, 1024])
                    # print(f"Single image tensor shape: {p_c.shape}") 

                # æœ€ç»ˆå †å æˆ Batch Tensor: [B, C, T, H, W]
                # **ä½¿ç”¨ torch.stack è‡ªåŠ¨åœ¨ç¬¬ 0 ç»´æ·»åŠ  Batch size**
                vae_input_content = torch.stack(processed_vae_images_content).to(accelerator.device, dtype=weight_dtype)
                vae_input_style = torch.stack(processed_vae_images_style).to(accelerator.device, dtype=weight_dtype)
                vae_input_gt = torch.stack(processed_vae_images_gt).to(accelerator.device, dtype=weight_dtype)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æœ€ç»ˆ Batch å½¢çŠ¶æ˜¯å¦æ˜¯ [B, C, T, H, W] (ä¾‹å¦‚ [1, 3, 1, 1024, 1024])
                print(f"Final VAE input shape (Content): {vae_input_content.shape}")

                # å¦‚æœè¿™é‡Œå½¢çŠ¶ä»ç„¶æ˜¯ 6Dï¼Œè¯´æ˜åœ¨æŸä¸ªéšè—çš„è§’è½äº§ç”Ÿäº†å¤šä½™çš„ç»´åº¦ã€‚
                # æœ€å¼ºåŠ›ä¿®æ­£ï¼šå¦‚æœå½¢çŠ¶ > 5Dï¼Œå¼ºåˆ¶ç§»é™¤å¤šä½™çš„ Batch ç»´åº¦ B2 (é€šå¸¸æ˜¯ç¬¬ 1 ç»´)
                if vae_input_content.ndim > 5:
                    # æˆ‘ä»¬å‡è®¾å¤šå‡ºæ¥çš„ B2 ç»´åº¦æ˜¯ 1ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ç§»é™¤ (squeeze)
                    vae_input_content = vae_input_content.squeeze(1)
                    vae_input_style = vae_input_style.squeeze(1)
                    vae_input_gt = vae_input_gt.squeeze(1)
                    print(f"Corrected VAE input shape (Content): {vae_input_content.shape}")
                # è°ƒè¯•è¯­å¥ (å¯ç§»é™¤)
                print(f"Final VAE input shape: {vae_input_content.shape}")
                prompt_embeds = batch["prompt_embeds"].to(dtype=weight_dtype)
                prompt_embeds_mask = batch["prompt_embeds_mask"]

                # 2. å‡†å¤‡ Latents (åˆ©ç”¨ pipeline.prepare_latents çš„é€»è¾‘)
                # ä½ çš„ prepare_latents è´Ÿè´£ VAE encode å’Œ Packingï¼Œå¹¶è¿”å› indices
                # ä½†å®ƒå†…éƒ¨å‡è®¾è¦ç”Ÿæˆéšæœº latentsã€‚æˆ‘ä»¬éœ€è¦è¿™é‡Œä¼ å…¥ GT latents ä½œä¸º baseã€‚
                # æ‰€ä»¥æˆ‘ä»¬åªèƒ½å€Ÿç”¨ _encode_vae_image å’Œ _pack_latentsï¼Œè‡ªå·±ç»„è£…æµç¨‹ï¼Œ
                # å¦åˆ™ç›´æ¥è°ƒ prepare_latents æ¯”è¾ƒéš¾å¡å…¥ GT å›¾åƒä½œä¸º "Noise" çš„åŸºç¡€ã€‚
                
                bsz = len(content_imgs)
                print(f"transformer config: {transformer.config}")
                # num_channels_latents = transformer.config['in_channels'] // 4

                with torch.no_grad():
                    # Encode Content & Style (Condition)
                    # _encode_vae_image å¤„ç†äº† retrieve_latents å’Œ norm
                    content_latents = pipeline._encode_vae_image(vae_input_content, generator=None)
                    style_latents = pipeline._encode_vae_image(vae_input_style, generator=None)
                    gt_latents_raw = pipeline._encode_vae_image(vae_input_gt, generator=None)
                    
                    # Pack Latents
                    # éœ€è¦è·å– latent çš„ H, Wã€‚ _encode_vae_image è¿”å› [B, C, 1, H, W]
                    l_h, l_w = gt_latents_raw.shape[3], gt_latents_raw.shape[4]

                    # Packing Logic (Directly calling static method or instance method)
                    packed_content = pipeline._pack_latents(content_latents, bsz, num_channels_latents, l_h, l_w)
                    packed_style = pipeline._pack_latents(style_latents, bsz, num_channels_latents, l_h, l_w)
                    packed_gt = pipeline._pack_latents(gt_latents_raw, bsz, num_channels_latents, l_h, l_w)
                    
                    L_content = packed_content.shape[1]
                    L_style = packed_style.shape[1]
                    L_noise = packed_gt.shape[1]

                # 3. Add Noise (Training Specific)
                noise = torch.randn_like(packed_gt) # åœ¨ Packed ç©ºé—´åŠ å™ªï¼Œæˆ–è€…åœ¨ Latent ç©ºé—´åŠ å™ªå† Pack æ˜¯ä¸€æ ·çš„
                # ä¸ºäº†ä¸¥è°¨ï¼Œæˆ‘ä»¬åœ¨ Packed ç©ºé—´åš Flow Matching
                
                u = compute_density_for_timestep_sampling(weighting_scheme="none", batch_size=bsz, logit_mean=0.0, logit_std=1.0, mode_scale=1.29)
                indices = (u * noise_scheduler.config.num_train_timesteps).long()
                timesteps = noise_scheduler.timesteps[indices].to(accelerator.device)
                
                sigmas = get_sigmas(timesteps, n_dim=packed_gt.ndim, dtype=packed_gt.dtype)
                packed_noisy_input = (1.0 - sigmas) * packed_gt + sigmas * noise
                
                # 4. æ„é€  Input & Attention Kwargs (å®Œå…¨å¯¹é½ Pipeline)
                # Pipeline Logic: latent_model_input = cat([latents(noise), content, style])
                hidden_states = torch.cat([packed_noisy_input, packed_content, packed_style], dim=1)
                
                # Indices logic
                style_start_idx = L_noise + L_content
                style_end_idx = style_start_idx + L_style

                attention_kwargs = {
                    "style_image_latents": packed_style,
                    "style_start_idx": style_start_idx,
                    "style_end_idx": style_end_idx,
                    "noise_patches_length": L_noise,
                    "content_patches_length": L_content,
                    "style_scale": 1.0
                }

                # 5. æ„é€  RoPE img_shapes
                # Pipeline: img_shapes = [[(1, h, w), (1, vh, vw)...]]
                # å¯¹äºè®­ç»ƒï¼Œhidden_states åŒ…å« Noise(GT size), Content, Style
                # å‡è®¾è¿™ä¸‰è€…åœ¨ VAE ç¼–ç åå°ºå¯¸ä¸€è‡´ (éƒ½ç»è¿‡ resize åˆ° resolution)
                # packed åå°ºå¯¸è¦é™¤ä»¥ 2
                p_h, p_w = l_h // 2, l_w // 2
                # å¯¹åº” [Noise, Content, Style]
                img_shapes = [[(1, p_h, p_w), (1, p_h, p_w), (1, p_h, p_w)]] * bsz
                
                txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()

                # 6. Forward
                if args.checkpointing_steps:
                    transformer.enable_gradient_checkpointing()
                
                model_pred = transformer(
                    hidden_states=hidden_states,
                    timestep=timesteps / 1000,
                    encoder_hidden_states=prompt_embeds,
                    encoder_hidden_states_mask=prompt_embeds_mask,
                    img_shapes=img_shapes,
                    txt_seq_lens=txt_seq_lens,
                    attention_kwargs=attention_kwargs,
                    return_dict=False,
                )[0]

                # 7. Loss
                # æå– Noise éƒ¨åˆ†çš„è¾“å‡º
                model_pred_noise = model_pred[:, :L_noise]
                target = noise - packed_gt

                weighting = compute_loss_weighting_for_sd3(weighting_scheme="none", sigmas=sigmas)
                loss = torch.mean(
                    (weighting.float() * (model_pred_noise.float() - target.float()) ** 2).reshape(target.shape[0], -1),
                    1,
                )
                loss = loss.mean()

                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()

            if accelerator.sync_gradients:
                global_step += 1
                print(f"Step {global_step}: Loss {loss.item()}")

                if global_step % args.checkpointing_steps == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    os.makedirs(save_path, exist_ok=True)
                    # åªä¿å­˜åŒ…å« style_ çš„å‚æ•°
                    unwrapped = accelerator.unwrap_model(transformer)
                    state_dict = unwrapped.state_dict()
                    style_weights = {k: v for k, v in state_dict.items() if "style_" in k}
                    torch.save(style_weights, os.path.join(save_path, "style_kv_weights.pt"))
                    logger.info(f"Saved style weights to {save_path}")

            if global_step >= args.max_train_steps:
                break
    
    accelerator.end_training()

if __name__ == "__main__":
    main()